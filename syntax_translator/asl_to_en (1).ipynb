{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7WwrKgWHQE_"
      },
      "outputs": [],
      "source": [
        "!pip install sacrebleu\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install transformers --upgrade\n",
        "!pip install peft\n",
        "!pip install accelerate -U >=0.20.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzP9_BgKHRO0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, AutoTokenizer\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from huggingface_hub import notebook_login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hjCu1liKR5q"
      },
      "outputs": [],
      "source": [
        "sentences = load_dataset('angelacao/gloss_to_spoken5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trASWJqFtPTQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "checkpoint = \"facebook/bart-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "source_lang = \"asl\"\n",
        "target_lang = \"en\"\n",
        "prefix = \"translate ASL to spoken English: \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgeSpLr1pNV1"
      },
      "outputs": [],
      "source": [
        "train_df = sentences[\"train\"]\n",
        "test_df = sentences[\"test\"]\n",
        "validation_df = sentences[\"validation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwJNIWmUtLZd"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
        "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_train = train_df.map(preprocess_function, batched=True)\n",
        "tokenized_validation = validation_df.map(preprocess_function, batched=True)\n",
        "tokenized_test = test_df.map(preprocess_function, batched=True)\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRUYc4sBIQGb"
      },
      "outputs": [],
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJiXtN6aHrlJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kMPYq5CHNmI"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "\n",
        "lora_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, r=64, lora_dropout=0.05, lora_alpha=128)\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cab846nTrHwj"
      },
      "outputs": [],
      "source": [
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UULXUBSbIjq_"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"ASL_to_Spoken\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=100,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=5,\n",
        "    predict_with_generate=True,\n",
        "    push_to_hub=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_validation,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)],\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-aq3qh8H4sv"
      },
      "outputs": [],
      "source": [
        "results = trainer.evaluate(tokenized_test)\n",
        "\n",
        "print(\"Evaluation Results:\", results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMRKlN5CH7At"
      },
      "outputs": [],
      "source": [
        "\n",
        "loaded_model = AutoModelForSeq2SeqLM.from_pretrained(\"angelacao/ASL_to_Spoken\")\n",
        "\n",
        "sample_input = \"translate ASL to spoken English: YOU WEDDING WHERE\"\n",
        "\n",
        "sample_input_ids = tokenizer.encode(sample_input, return_tensors=\"pt\")\n",
        "output_ids = loaded_model.generate(sample_input_ids)\n",
        "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Translated Output:\", output_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}